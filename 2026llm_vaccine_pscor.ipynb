{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dbcc544-89f8-4ed4-8012-282b466d4ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install unsloth\n",
    "# !pip install huggingface\n",
    "# !pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d1274c-0a5e-45c0-99e0-7e068858c213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.12.9: Fast Qwen3 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    Tesla V100-SXM2-16GB. Num GPUs = 1. Max memory: 15.766 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd8b0c946c442cca4e754485c76c08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5aabac3ec5462bbb6f8115ebd8eee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe75fd6c129a491386a1fc67a48687c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24887651b29c404c818103f81dafe782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/1.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44cf7f1c1294fe98200c1f42ab57687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517171ad846e474e9a4dd4ad8e4f8bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83034ad46ec346be8eb4e78c85deaa58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f200f14659b403e867d4c70fd3ed376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd58f2d2c0bf4643bb48ea9e12389fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0fd6887bdd46f79d0bd05a9fd0c5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609bec87b41d466d8c34d902031b52e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60c87d2c7b94616ae214a4c416745f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f007a768ec419d8cd8ff384d52b635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a094c5860444778a333be4332db55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "fourbit_models = [\n",
    "    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n",
    "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n",
    "\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Phi-4\",\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "model_name = \"unsloth/Qwen3-14B-unsloth-bnb-4bit\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # or torch.float16 if no bf16\n",
    "    llm_int8_enable_fp32_cpu_offload=True,  # key for offloading\n",
    ")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=8048,          # consider lowering if you still OOM\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",            # let HF shard between GPU/CPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2189972-8e78-458a-9028-c49376b9dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "datafold = \"/sbgenomics/project-files/\"\n",
    "file = \"y_pasc_score2024.csv\"\n",
    "# file = \"RECOVERAdult_BiostatsDerived_202412_symptoms_deID.csv\"\n",
    "result = pd.read_csv(datafold + file)\n",
    "result = result.rename(columns={\"PARTICIPANT_ID\":\"id\", \"pasc_score_2024\":\"name\"})\n",
    "result['index'] = \"pasc_score_2024\"\n",
    "result_reordered = result.iloc[:, [0, 3,1,2]]\n",
    "# file = \"all_methods_summary.csv\"\n",
    "file = \"vaccine_2026.csv\"\n",
    "df = pd.read_csv(datafold + file)\n",
    "df=df.iloc[:, [1, 2, 3,4]]\n",
    "combined_df = pd.concat([result_reordered, df], ignore_index=True, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5555118-20d6-4b02-a7cf-5f09de672462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RA11305</td>\n",
       "      <td>pasc_score_2024</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RA11305</td>\n",
       "      <td>pasc_score_2024</td>\n",
       "      <td>2021-04-03</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RA11305</td>\n",
       "      <td>pasc_score_2024</td>\n",
       "      <td>2021-06-28</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RA11305</td>\n",
       "      <td>pasc_score_2024</td>\n",
       "      <td>2021-09-28</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RA11305</td>\n",
       "      <td>pasc_score_2024</td>\n",
       "      <td>2021-12-23</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id            index        date  name\n",
       "0  RA11305  pasc_score_2024  2021-01-01  16.0\n",
       "1  RA11305  pasc_score_2024  2021-04-03  14.0\n",
       "2  RA11305  pasc_score_2024  2021-06-28   8.0\n",
       "3  RA11305  pasc_score_2024  2021-09-28  13.0\n",
       "4  RA11305  pasc_score_2024  2021-12-23  10.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1a858ee-6586-42dd-bf16-fae6b0c7e671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13451\n",
      "(313, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# B = \"pasc_score_2024\"  # set your target value\n",
    "# out = combined_df.groupby(\"id\").filter(lambda g: (g[\"index\"] == B).sum() > 4)\n",
    "# print(out.shape)#282581, 4)\n",
    "\n",
    "ids = np.unique(out['id'])\n",
    "print(len(ids)) #15158 13451\n",
    "index = 5\n",
    "\n",
    "selected_ids = [id_value for id_value in ids[5:20]] # Make sure ids is just the list of values\n",
    "output_ind_df = combined_df[combined_df['id'].isin(selected_ids)]\n",
    "\n",
    "# output_ind_df = combined_df[combined_df['id']==ids[index]]\n",
    "print(output_ind_df.shape)\n",
    "# df_new = output_ind_df.drop(columns=['id'])\n",
    "output_ind = output_ind_df.to_string(index=True, header=True)\n",
    "# output_string = combined_df.to_string(index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71a4b63c-e9c6-4542-8aaa-474e91160bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_ind_df\n",
    "prompt = (\n",
    "\"\"\"You are a clinical, medical, physician, and statistical expert.\n",
    "\n",
    "You will be given a table for several patient with columns:\n",
    "- id: patient id\n",
    "- date: date of record\n",
    "- index: 0=enrollment; 1/2/3/4=vaccine dose number; \"followup\" or \"followup_k\"=follow-up visit, pasc=wellness score (lower is better). Threshold: >=12 = Long COVID (PASC), <12 = No PASC.\n",
    "- name: free-text description (may include vaccine brand like pfizer/moderna/etc, and may contain notes, if it is value, then pasc)\n",
    "- (optional) followup_2: \"no\" means no vaccine between this visit and the previous visit; \"yes\" means a vaccine occurred between visits.\n",
    "\n",
    "Important notes:\n",
    "- Vaccines may occur before enrollment (index=0).\n",
    "- Dates may be out of order in the raw text; sort by date when building a timeline.\n",
    "- If vaccine brand is ambiguous/misspelled (e.g., \"pfzier\"), normalize to the closest common brand and also keep the raw string.\n",
    "\n",
    "TASKS\n",
    "Write a concise summary,sStart a section exactly titled: #Summary\n",
    "   In #Summary include:\n",
    "   - 3–5 sentences describing the patients’ course, key events, and strongest observed associations of the longitudal information\n",
    "\n",
    "INPUT TABLE :\n",
    "\"\"\"\n",
    "+ output_ind +\n",
    "\"\"\"\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cda6b19-4803-430c-aa1f-6b239cfadc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prompt = \"here is the vaccine record of one patient, can you provide a summary of the useful information \" +output_string + \" and present results in a table to be saved, such as date of vaccine, index of vaccine, name of vaccine\"\n",
    "# prompt = \"here is followup of maybe long covid patient, vaccines may be taken before enrollment. there are 4 columns, id column: is the patient id, date column: is the date of item recorded, index column: 0 means the enrollment, 1,2,3,4 means the first, second, third and fourth vaccine, followup means the followup, pasc_score_2024 means the score of wellness, the lower the score, the better the patient, 12 is usually the threshold, >=12 means long covid, < 12 means no; name column: is the description, such as pfzier is the vaccine name, number with pasc_score_2024 means the score, with index column followup_2, no means there is no vaccine between this visit and previous visit, yes means another vaccine \" +\\\n",
    "# output_ind + \" make analysis of the table, then find some discoveries of such as pasc score and the vaccine, the pattern, the timeline, when peak happens, what are the events before and after, discories that usually in nature/science. Write summary in #Summary\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    temperature = 0.7,\n",
    "    max_new_tokens = 3024,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a0a96-4b79-449c-b01c-6e4c901cd305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # pip install accelerate\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", device_map=\"auto\")\n",
    "\n",
    "# input_text = \"Write me a poem about Machine Learning.\"\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(**input_ids)\n",
    "# print(tokenizer.decode(outputs[0]))\n",
    "### **Conclusion**\n",
    "- Vaccines (Pfizer/Moderna) may help reduce long COVID symptoms (PASC scores ≤12), but outcomes vary by individual.  \n",
    "- Peaks in PASC scores often occur 6–12 months post-enrollment, followed by recovery in many cases.  \n",
    "- Long-term followups (≥3 years) show that some patients achieve sustained recovery, even without additional vaccines.  \n",
    "\n",
    "This analysis highlights the complex interplay between vaccination, immune response, and long-term health outcomes in post-acute sequelae of SARS-CoV-2 infection.<|im_end|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900574c4-2c2e-4392-b99f-6dd9e221fae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1195a876-3665-41b7-b1e3-005f8d82e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "# encoding/encoding_dsv32.py\n",
    "from encoding_dsv32 import encode_messages, parse_message_from_completion_text\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V3.2\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"hello\"},\n",
    "    {\"role\": \"assistant\", \"content\": prompt},#\"Hello! I am DeepSeek.\", \"reasoning_content\": \"thinking...\"},\n",
    "    {\"role\": \"user\", \"content\": \"1+1=?\"}\n",
    "]\n",
    "encode_config = dict(thinking_mode=\"thinking\", drop_thinking=True, add_default_bos_token=True)\n",
    "\n",
    "# messages -> string\n",
    "prompt = encode_messages(messages, **encode_config)\n",
    "# Output: \"<｜begin▁of▁sentence｜><｜User｜>hello<｜Assistant｜></think>Hello! I am DeepSeek.<｜end▁of▁sentence｜><｜User｜>1+1=?<｜Assistant｜><think>\"\n",
    "\n",
    "# string -> tokens\n",
    "tokens = tokenizer.encode(prompt)\n",
    "# Output: [0, 128803, 33310, 128804, 128799, 19923, 3, 342, 1030, 22651, 4374, 1465, 16, 1, 128803, 19, 13, 19, 127252, 128804, 128798]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f133c8-1b90-4db1-8d2c-4c4e7701d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "**PASC Score Fluctuations Over Time**:\n",
    "**Possible Biological Insights**:\n",
    "   - **Waning Immunity**: The peak in 2024, 20 months after the last vaccine, may reflect **diminished protection** from prior doses, leading to a resurgence of symptoms.\n",
    "   - **Immune Memory and Long COVID**: The patient’s PASC score fluctuated, suggesting **variable immune response**. The 2024 peak might indicate **reduced immune memory** or **viral reactivation** (e.g., SARS-CoV-2 variants or other pathogens).\n",
    "   - **Seasonal or Environmental Factors**: The spike in 2024 could correlate with **seasonal viral activity** (e.g., winter 2023–2024) or **environmental stressors**.\n",
    "\n",
    "5. **Clinical Implications**:\n",
    "   - **Long-Term Monitoring**: The case highlights the need for **long-term monitoring** of patients post-vaccination, especially those with prior long COVID.\n",
    "   - **Vaccine Efficacy Over Time**: The peak in 2024 suggests that **vaccine protection may not fully prevent long-term symptoms**, especially in vulnerable individuals.\n",
    "   - **Therapeutic Window**: The drop in PASC score to 2.0 in May 2024 suggests **possible recovery** or **intervention** (not documented here), indicating variability in disease progression.\n",
    "\n",
    "\n",
    "                                                           ### **6. Scientific Insights**\n",
    "- **Long COVID is not static**:  \n",
    "  - Patients may experience **remissions and relapses**, even with prior vaccinations.  \n",
    "  - **PASC scores are not predictive of long-term outcomes** without additional biomarkers.  \n",
    "\n",
    "- **Vaccine Efficacy for Long COVID**:  \n",
    "  - While vaccines may reduce acute symptoms, **they do not eliminate Long COVID risk** in all individuals.  \n",
    "\n",
    "- **Need for Longitudinal Studies**:  \n",
    "  - More data on **vaccine timing, booster doses, and comorbidities** are needed to understand Long COVID dynamics.  \n",
    "\n",
    "---\n",
    "\n",
    "### **7. Recommendations for Further Research**\n",
    "- Investigate **seasonal triggers** (e.g., viral infections, pollution) in Long COVID relapses.  \n",
    "- Explore **biomarkers** (e.g., cytokine profiles, autoantibodies) to predict PASC score fluctuations.  \n",
    "- Study **vaccine boosters** in Long COVID patients to assess sustained protection.  \n",
    "- Address **missing data** in follow-up visits to improve outcome prediction models.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "Patient PASC scores demonstrate **significant variability**, with a **sharp initial improvement** post-enrollment, followed by **recurrent Long COVID symptoms**. Pre-vaccination history and the absence of further vaccines suggest that **Long COVID is not fully mitigated by vaccination alone**. The **peak in September 2023** highlights the need for ongoing monitoring and personalized interventions for Long COVID patients.<|im_end|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cc7e46c-4ded-4d64-b868-5ffe3e55c3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057507767c17411fb67ebaa025564cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04537b0b8e794c80a8155a61a78708d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d12e5ae17448d4914b1790403ef677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c951b1664ce54a8cbb255d6c344b1c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad43ee1c10a4604b6bebefab0a5c998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7bdcc159814df7bc74715fa12ced11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'nvidia/Nemotron-Cascade-8B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "'''\n",
    "single-turn example\n",
    "'''\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"calculate 1+1?\"}\n",
    "]\n",
    "\n",
    "# thinking mode\n",
    "prompt_thinking = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=True)\n",
    "# prompt_thinking = '<|im_start|>system\\nYou are a helpful and harmless assistant.<|im_end|>\\n<|im_start|>user\\ncalculate 1+1? /think<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "\n",
    "# instruct mode\n",
    "prompt_instruct = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False)\n",
    "# prompt_instruct = '<|im_start|>system\\nYou are a helpful and harmless assistant.<|im_end|>\\n<|im_start|>user\\ncalculate 1+1? /no_think<|im_end|>\\n<|im_start|>assistant\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d576bb5-bcab-4bf6-bb95-f3bccbd740be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
