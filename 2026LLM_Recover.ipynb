{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326ba4f-4da5-47e3-a597-b9a05ca5c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth\n",
    "!pip install huggingface\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d1274c-0a5e-45c0-99e0-7e068858c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "fourbit_models = [\n",
    "    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n",
    "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n",
    "\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Phi-4\",\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "model_name = \"unsloth/Qwen3-32B-unsloth-bnb-4bit\"\n",
    "# model_name = \"unsloth/Phi-4\"#,#\"unsloth/Qwen3-32B-unsloth-bnb-4bit\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # or torch.float16 if no bf16\n",
    "    llm_int8_enable_fp32_cpu_offload=True,  # key for offloading\n",
    ")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=2048,          # consider lowering if you still OOM\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",            # let HF shard between GPU/CPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2189972-8e78-458a-9028-c49376b9dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "datafold = \"/sbgenomics/project-files/\"\n",
    "file = \"y_pasc_score2024.csv\"\n",
    "# file = \"RECOVERAdult_BiostatsDerived_202412_symptoms_deID.csv\"\n",
    "result = pd.read_csv(datafold + file)\n",
    "result = result.rename(columns={\"PARTICIPANT_ID\":\"id\", \"pasc_score_2024\":\"name\"})\n",
    "result['index'] = \"pasc_score_2024\"\n",
    "result_reordered = result.iloc[:, [0, 3,1,2]]\n",
    "# file = \"all_methods_summary.csv\"\n",
    "file = \"vaccine_2026.csv\"\n",
    "df = pd.read_csv(datafold + file)\n",
    "df=df.iloc[:, [1, 2, 3,4]]\n",
    "combined_df = pd.concat([result_reordered, df], ignore_index=True, axis=0)\n",
    "combined_df['index'] = combined_df['index'].replace('pasc_score_2024', 'pasc')\n",
    "rows_to_keep_mask = ~((combined_df['index'] == 'pasc') & (combined_df['name'].isna()))\n",
    "\n",
    "# Filter the DataFrame using the mask\n",
    "combined_df = combined_df[rows_to_keep_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1a858ee-6586-42dd-bf16-fae6b0c7e671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250430, 4)\n",
      "11669\n",
      "(57, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "B = \"pasc\"  # set your target value\n",
    "out = combined_df.groupby(\"id\").filter(lambda g: (g[\"index\"] == B).sum() > 4)\n",
    "print(out.shape)#282581, 4)\n",
    "\n",
    "ids = np.unique(out['id'])\n",
    "print(len(ids)) #15158 13451\n",
    "index = 5\n",
    "\n",
    "selected_ids = [id_value for id_value in ids[5:8]] # Make sure ids is just the list of values\n",
    "output_ind_df = combined_df[combined_df['id'].isin(selected_ids)]\n",
    "\n",
    "# output_ind_df = combined_df[combined_df['id']==ids[index]]\n",
    "print(output_ind_df.shape)\n",
    "# df_new = output_ind_df.drop(columns=['id'])\n",
    "output_ind = output_ind_df.to_string(index=True, header=True)\n",
    "# output_string = combined_df.to_string(index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71a4b63c-e9c6-4542-8aaa-474e91160bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_ind_df\n",
    "prompt = (\n",
    "\"\"\"You are a clinical, medical, physician, and statistical expert.\n",
    "\n",
    "You will be given a table for several patient with columns:\n",
    "- id: patient id\n",
    "- date: date of record\n",
    "- index: 0=enrollment; 1/2/3/4=vaccine dose number; \"followup\" or \"followup_k\"=follow-up visit, pasc=wellness score (lower is better). Threshold: >=12 = Long COVID (PASC), <12 = No PASC.\n",
    "- name: free-text description (may include vaccine brand like pfizer/moderna/etc, and may contain notes, if it is value, then pasc)\n",
    "- (optional) followup_2: \"no\" means no vaccine between this visit and the previous visit; \"yes\" means a vaccine occurred between visits.\n",
    "\n",
    "Important notes:\n",
    "- Use feature selection to find most important patterns, do pairwise comparison\n",
    "- Data noise, vaccines may occur before enrollment (index=0).Dates may be out of order in the raw text; sort by date when building a timeline.\n",
    "- If vaccine brand is ambiguous/misspelled (e.g., \"pfzier\"), normalize to the closest common brand and also keep the raw string.\n",
    "- Consider fairness, treat each participant equally. consider feature selection, choose the dominant patterns, perform pairwise comparison\n",
    "\n",
    "TASKS\n",
    "Write a concise summary,sStart a section exactly titled: #Summary\n",
    "   In #Summary include:\n",
    "   - 3–5 sentences describing the patients’ course, key events, and strongest observed associations of the longitudal information\n",
    "\n",
    "INPUT TABLE :\n",
    "\"\"\"\n",
    "+ output_ind +\n",
    "\"\"\"\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cda6b19-4803-430c-aa1f-6b239cfadc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prompt = \"here is the vaccine record of one patient, can you provide a summary of the useful information \" +output_string + \" and present results in a table to be saved, such as date of vaccine, index of vaccine, name of vaccine\"\n",
    "# prompt = \"here is followup of maybe long covid patient, vaccines may be taken before enrollment. there are 4 columns, id column: is the patient id, date column: is the date of item recorded, index column: 0 means the enrollment, 1,2,3,4 means the first, second, third and fourth vaccine, followup means the followup, pasc_score_2024 means the score of wellness, the lower the score, the better the patient, 12 is usually the threshold, >=12 means long covid, < 12 means no; name column: is the description, such as pfzier is the vaccine name, number with pasc_score_2024 means the score, with index column followup_2, no means there is no vaccine between this visit and previous visit, yes means another vaccine \" +\\\n",
    "# output_ind + \" make analysis of the table, then find some discoveries of such as pasc score and the vaccine, the pattern, the timeline, when peak happens, what are the events before and after, discories that usually in nature/science. Write summary in #Summary\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    temperature = 0.7,\n",
    "    max_new_tokens = 3024,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a0a96-4b79-449c-b01c-6e4c901cd305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900574c4-2c2e-4392-b99f-6dd9e221fae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d576bb5-bcab-4bf6-bb95-f3bccbd740be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
